{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RrFQuRoZsFvN3tkEYZbMHU",
     "type": "MD"
    }
   },
   "source": [
    "# 1 Download the dataset from the following link:\n",
    "https://www.kaggle.com/kingburrito666/shakespeare-plays/\n",
    "and replace the line of \"with open('data.txt', 'r') as file:\" with \"with open('alllines.txt', 'r') as file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wgfhGbtO2sxwE9Chh1cjHl",
     "type": "MD"
    }
   },
   "source": [
    "# 2 - Read the Text\n",
    "To read the text file of lines from Shakespeare plays, use these lines of code:\n",
    "#Read the text file as separate lines of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4VvDnqHbqTz57GhIcv6Pz1",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     lines \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Define words, vocabulary size and sequences of words as lines\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_to_word_sequence, Tokenizer\n\u001b[1;32m      7\u001b[0m words \u001b[38;5;241m=\u001b[39m text_to_word_sequence(text)\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "with open('alllines.txt', 'r') as file:\n",
    "\n",
    "    text = file.read()\n",
    "    lines = text.lower().split('\\n')\n",
    "#Define words, vocabulary size and sequences of words as lines\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "words = text_to_word_sequence(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(words)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "#Find subsequences \n",
    "subsequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "       subsequence = sequence[:i+1]\n",
    "       subsequences.append(subsequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RREJiY7qZC8jrogpMnUWcm",
     "type": "MD"
    }
   },
   "source": [
    "# 3 - Padding Your Sequences\n",
    "You need to have equal sequences for training; for this, you will apply padding.\n",
    "Write these lines of code to implement the padding needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GjypHZdQI097glMAVmxbfv",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequence_length = max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(subsequences, maxlen=sequence_length, padding='pre')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "AXMXiurnRD3sSj0Vd6F18x",
     "type": "MD"
    }
   },
   "source": [
    "# 4 - Encode the Target Labels\n",
    "Use these lines of code to encode your labels for training:  \n",
    "\n",
    "- from keras.utils import to_categorical\n",
    "- x, y = sequences[:,:-1],sequences[:,-1]\n",
    "- y = to_categorical(y, num_classes=vocabulary_size)  \n",
    "\n",
    "Define an RNN with the following layers:  \n",
    "\n",
    "An embedding layer with the following parameters:  \n",
    "The input dimension is vocabulary_size  \n",
    "The output dimension is 100  \n",
    "The input length is sequence_length - 1  \n",
    "An LSTM layer with 100 units  \n",
    "A dropout layer with a dropout rate of 10%  \n",
    "\n",
    "A dense layer with the following parameters:  \n",
    "Activation function is softmax  \n",
    "The number of units is vocabulary_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "OJKA3cUMFnbALIhRpYmZ5N",
     "type": "MD"
    }
   },
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wUyPIYBDECZGSbk6vH2h0M",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "x, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RKUPGw2NzOGWG9FQu5FVMz",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "KYKYdMhfQ1xfLpZylwucpY",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=100, input_length=sequence_length - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('model.h5', monitor='loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(x, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
